{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt   #for data visualization and graphical plotting\nfrom matplotlib import style      #for styling the plot\nstyle.use(\"ggplot\")\n\nimport nltk\nfrom nltk.util import pr\nfrom nltk.tokenize import word_tokenize      #to divide strings into lists of substrings\nfrom nltk.stem import WordNetLemmatizer      #to link words with similar meanings to one word.\nfrom nltk.corpus import stopwords            #to filterout useless data\nstopword = set(stopwords.words('english'))\n\nfrom wordcloud import WordCloud\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression  \nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n\nimport re\nimport string","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!unzip /usr/share/nltk_data/corpora/wordnet.zip -d /usr/share/nltk_data/corpora/","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Importing the Dataset","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/hate-speech-and-offensive-language-dataset/labeled_data.csv\")\ndata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Visualization","metadata":{}},{"cell_type":"code","source":"# to get more info about dataset\ndata.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Adding  a new column to this dataset as labels which will contain the values as:\n\n1. Hate Speech \n2. Offensive Language \n3. No Hate and Offensive","metadata":{}},{"cell_type":"code","source":"data[\"labels\"] = data[\"class\"].map({0: \"Hate Speech\", \n                                    1: \"Offensive Language\", \n                                    2: \"No Hate and Offensive\"})\nprint(data.head())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# analysing some data\nprint(data[\"tweet\"].iloc[0],\"\\n\")\nprint(data[\"tweet\"].iloc[1],\"\\n\")\nprint(data[\"tweet\"].iloc[2],\"\\n\")\nprint(data[\"tweet\"].iloc[3],\"\\n\")\nprint(data[\"tweet\"].iloc[4],\"\\n\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we only select the tweet and labels columns for the rest of the task of training a hate speech detection model:","metadata":{}},{"cell_type":"code","source":"data = data[[\"tweet\", \"labels\"]]\ndata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Preprocessing","metadata":{}},{"cell_type":"code","source":"def clean(text):\n    text = str(text).lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub(r\"\\@w+|\\#\",'',text)\n    text = re.sub(r\"[^\\w\\s]\",'',text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    tweet_tokens = word_tokenize(text)\n    filtered_tweets=[w for w in tweet_tokens if not w in stopword] #removing stopwords\n    return \" \".join(filtered_tweets)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#applying pre-processing to text data\ndata.tweet=data['tweet'].apply(clean)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# removing duplicate data\ntweetData = data.drop_duplicates(\"tweet\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# to see updated number of columns as duplicates entries are removed\ntweetData.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lemmatizer=WordNetLemmatizer()\ndef lemmatizing(data):\n    tweet=[lemmatizer.lemmatize(word) for word in data]\n    return data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#lemmatizing the processed data\ntweetData['tweet']=tweetData['tweet'].apply(lambda x: lemmatizing(x))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# to see the data after pre-processing\nprint(tweetData[\"tweet\"].iloc[0],\"\\n\")\nprint(tweetData[\"tweet\"].iloc[1],\"\\n\")\nprint(tweetData[\"tweet\"].iloc[2],\"\\n\")\nprint(tweetData[\"tweet\"].iloc[3],\"\\n\")\nprint(tweetData[\"tweet\"].iloc[4],\"\\n\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# to see distribution of labels\ntweetData['labels'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# to visualize the data using a count plot\nfig = plt.figure(figsize=(5,5))\nax = sns.countplot(x='labels', data=tweetData)\n\nax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha=\"right\")\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# visualizing data using piechart\nfig = plt.figure(figsize=(7,7))\ncolors = ('red', 'green', 'blue')\nwp = {'linewidth':2, \"edgecolor\":'black'}\ntags = tweetData['labels'].value_counts()\nexplode=(0.1,0.1,0.1)\ntags.plot(kind='pie', autopct=\"%1.1f%%\", shadow=True, colors=colors, startangle=90, wedgeprops=wp, explode=explode, label='')\nplt.title(\"Distribution of sentiments\")\n\n#here 0 indicates hate speech, 1 indicates offensive Language & 2 indicates no hate & offensive","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# visualizing non-hate tweets\nnon_hate_tweets = tweetData[tweetData.labels=='No Hate and Offensive']\nnon_hate_tweets.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"non_hate_tweets.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# visualizing non-hate tweets using word cloud\ntext=''.join([word for word in non_hate_tweets['tweet']])\nplt.figure(figsize=(20,15), facecolor='None')\nwordcloud=WordCloud(max_words=500, width=1600, height=800).generate(text)\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\nplt.title(\"Most frequent words in non hate tweets\", fontsize=19)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**TF - IDF: Term frequence - Inverse Document Frequency**\n\n    TF-IDF = TF(t,d) x IDF(t),\n    where TF(t,d) = number of times term \"t\" appears in a document \"d\".\n          IDF(t) = inverse document frequency of the term t\n\nThe TF-IDF vectorizer converts a collection of raw documents into a matrix of TF-IDF features.\n\n* **Bigram model**: provides probability of the next word given the past two words.\n* **Trigram model**: provides probability of the next word based on past three words.\n* **N-gram model**: provides probability based on past N-words.","metadata":{}},{"cell_type":"markdown","source":"### TF-IDF Bigram Model","metadata":{}},{"cell_type":"code","source":"#vectorizing the text data using TfidVectorizer and create a bigram lanuage model\nvect=TfidfVectorizer(ngram_range=(1,2)).fit(tweetData['tweet'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_names=vect.get_feature_names_out()\nprint(\"Number of features: {}\\n\", format(len(feature_names)))\nprint(\"First 200 features: \\n\", format(feature_names[:20]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### TF-IDF Trigram Model","metadata":{}},{"cell_type":"code","source":"#creating a trigram language model\nvect=TfidfVectorizer(ngram_range=(1,3)).fit(tweetData['tweet'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_names=vect.get_feature_names_out()\nprint(\"Number of features: {}\\n\", format(len(feature_names)))\nprint(\"First 200 features: \\n\", format(feature_names[:20]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Split Dataset","metadata":{}},{"cell_type":"code","source":"#separating the data into x and y to build the model\nX = tweetData['tweet']\nY = tweetData['labels']\nX = vect.transform(X) #transforming the x data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#printing the size of training and testing data\nprint(\"Size of X_train\", (X_train.shape))\nprint(\"Size of Y_train\", (Y_train.shape))\nprint(\"Size of X_test\", (X_test.shape))\nprint(\"Size of Y_test\", (Y_test.shape))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Training & Testing","metadata":{}},{"cell_type":"markdown","source":"### 1. Logistic Regression","metadata":{}},{"cell_type":"code","source":"#for training the data on logistic regression model\nlogreg = LogisticRegression()\nlogreg.fit(X_train, Y_train) #loading x_train and y_train data on model\nlogreg_predict = logreg.predict(X_test) #predicting the value for test data\nlogreg_acc = accuracy_score(logreg_predict, Y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Test accuracy: {:.2f}%\".format(logreg_acc*100)) #printing accuracy of the model","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#printing confusion matrix and classification report\nprint(confusion_matrix(Y_test, logreg_predict))\nprint(\"\\n\")\nprint(classification_report(Y_test, logreg_predict))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#for visualizing confusion matrix\nstyle.use('classic')\ncm = confusion_matrix(Y_test, logreg_predict, labels=logreg.classes_)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=logreg.classes_)\ndisp.plot()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Hyperparameter Tuning - GridSearchCV","metadata":{}},{"cell_type":"code","source":"#for performing hyperparameter tuning\nfrom sklearn.model_selection import GridSearchCV\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"param_grid = {'C':[100, 10, 1.0, 0.1, 0.01], 'solver' :['newton-cg', 'lbfgs','liblinear']}\ngrid = GridSearchCV(LogisticRegression(), param_grid, cv = 5)\ngrid.fit(X_train, Y_train)\nprint(\"Best Cross validation score: {:.2f}\".format(grid.best_score_))\nprint(\"Best parameters: \", grid.best_params_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"log_grid_pred = grid.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"log_grid_acc = accuracy_score(log_grid_pred, Y_test)\nprint(\"Test accuracy: {:.2f}%\".format(log_grid_acc*100)) #printing model accuracy after applying hyperparamenter tuning\nTest accuracy: 94.89%","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(confusion_matrix(Y_test, log_grid_pred))\nprint(\"\\n\")\nprint(classification_report(Y_test, log_grid_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2. Decision Tree Classifier","metadata":{}},{"cell_type":"code","source":"#for training the data on decision tree classifier model\ndtree = DecisionTreeClassifier()\ndtree.fit(X_train, Y_train) #loading x_train and y_train data on model\ndtree_predict = dtree.predict(X_test) #predicting the value for test data\ndtree_acc = accuracy_score(dtree_predict, Y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Test accuracy: {:.2f}%\".format(dtree_acc*100)) #printing accuracy of the model","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#printing confusion matrix and classification report\nprint(confusion_matrix(Y_test, dtree_predict))\nprint(\"\\n\")\nprint(classification_report(Y_test, dtree_predict))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#for visualizing confusion matrix\nstyle.use('classic')\ncm = confusion_matrix(Y_test, dtree_predict, labels=dtree.classes_)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=dtree.classes_)\ndisp.plot()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Deployment","metadata":{}},{"cell_type":"code","source":"def hate_speech_detection():\n    import streamlit as st\n    st.title(\"Hate Speech Detection\")\n    user = st.text_area(\"Enter any Tweet: \")\n    if len(user) < 1:\n        st.write(\"  \")\n    else:\n        sample = user\n        data = cv.transform([sample]).toarray()\n        a = clf.predict(data)\n        st.title(a)\nhate_speech_detection()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}